{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFormat: term_i, df_i:\\n    <doc1, tf_1: <pos1, pos2, … >; \\n    doc2, tf_2: <pos1, pos2, …>; …>\\n\\nFormat: to, 993427: \\n    <1, 6: <7, 18, 33, 72, 86, 231>;   \\n    2, 5: <1, 17, 74, 222, 255>; … >\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Format: term_i, df_i:\n",
    "    <doc1, tf_1: <pos1, pos2, … >; \n",
    "    doc2, tf_2: <pos1, pos2, …>; …>\n",
    "\n",
    "Format: to, 993427: \n",
    "    <1, 6: <7, 18, 33, 72, 86, 231>;   \n",
    "    2, 5: <1, 17, 74, 222, 255>; … >\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = eval(open('all_docs_freq_all_tokens_dict.txt','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'13185,1': [0]}, {'13191,8': [841, 848, 850, 948, 955, 969, 980, 982]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index['usernam, 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://mathpretty.com/10661.html\n",
    "\n",
    "NUM_OF_ALL_DOCS = 13173 # number of all documents (processed)\n",
    "\n",
    "def calculate_weight(raw_count_in_doc, term_doc_in_all_document):\n",
    "    \"\"\"\n",
    "    raw_count_in_doc : appear times of term in the doc (raw count)\n",
    "    term_doc_in_all_document : number of docs which contains the term (processed)\n",
    "    \"\"\"\n",
    "    idf = NUM_OF_ALL_DOCS / term_doc_in_all_document\n",
    "    w_i_j = (1 + math.log10(raw_count_in_doc)) * math.log10(idf)\n",
    "\n",
    "    return w_i_j\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    assert len(vec_a) == len(vec_b), \"Vectors needs to be same length\"\n",
    "\n",
    "    dot = sum(i * j for i, j in zip(vec_a, vec_b))\n",
    "    a_norm = sum(i**2 for i in vec_a) ** 0.5\n",
    "    b_norm = sum(i**2 for i in vec_b) ** 0.5\n",
    "\n",
    "    cos_sim = dot / (a_norm * b_norm)\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80df356f2c294a9abe9b8ad19a5c4eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "term_doc_in_all_document = 0\n",
    "\n",
    "keys = list(inverted_index.keys())\n",
    "\n",
    "term_weight_doc_id_dict = dict() # {'usernam':{'13185': 8.79, '13191': 27.10}, 'password':{'13191': 30.52}, 'becom': {'13194': 16.73}}\n",
    "\n",
    "pb = tqdm(total=len(keys), nrows=4, position=0, leave=True)\n",
    "\n",
    "for term_df_key in keys:\n",
    "    term = re.findall('\\w+', term_df_key)[0] # 'usernam, 2' -> 'usernam'\n",
    "    term_doc_in_all_document = int(re.findall(', ([0-9]+)', term_df_key)[0]) # 'usernam, 2' -> 2\n",
    "\n",
    "    temp_dict = dict()\n",
    "\n",
    "    for i in range(len(inverted_index[term_df_key])):\n",
    "        doc_raw_count_and_posting = list(inverted_index[term_df_key][i].keys())[0] # [{'13185,1': [0]}, {'13191,8': ...}...]\n",
    "\n",
    "        doc_id = re.findall('([0-9]+),', doc_raw_count_and_posting)[0] # '13185,1' -> 13185 #doc的編號\n",
    "        raw_count_in_doc = int(re.findall(',([0-9]+)', doc_raw_count_and_posting)[0]) # '13185,1' -> 1 #在這個doc出現幾次\n",
    "\n",
    "        weight_i_j = calculate_weight(raw_count_in_doc, term_doc_in_all_document)\n",
    "\n",
    "        temp_dict[doc_id] = weight_i_j\n",
    "\n",
    "    pb.update(1) # update progress bar\n",
    "    pb.set_description('Calculate weight in term_weight_doc_id_dict: ', refresh=True)\n",
    "\n",
    "    term_weight_doc_id_dict[term] = temp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76fd3825c22404599b1d53e0ab3f848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrected_queries_by_terms: ['art', 'appl']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6426844ef7b7470ab3cd9724c75386f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[['13184', 0.7071067811865476], ['13194', 0.7071067811865475]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = input()\n",
    "queries = inp.split()\n",
    "print(f\"queries: {queries}\")\n",
    "\n",
    "terms = list(term_weight_doc_id_dict.keys())\n",
    "\n",
    "corrected_queries_by_terms = [] # 'user name' -> 'username'\n",
    "term_weight_for_tokenized_queries = {} # d_1, d_2, ..., d_j, {'13185': (8.79, 0.00), ...}\n",
    "all_docs_ids_for_tokenized_queries = set() # ('13191', '13200')\n",
    "\n",
    "q_vector = []\n",
    "search_results = []\n",
    "\n",
    "pb1 = tqdm(total=len(queries), nrows=4, position=0, leave=True)\n",
    "\n",
    "for query in queries:\n",
    "    query = stemmer.stem(query) # stemmize the query token\n",
    "\n",
    "    if query in term_weight_doc_id_dict:\n",
    "        corrected_queries_by_terms.append(query)\n",
    "    else:\n",
    "        # get most similar terms for user's query\n",
    "        search_range = [t for t in terms if t[0] == query[0]] # optimize: reduce the search range by starts with same character\n",
    "\n",
    "        for i in range(len(search_range)):\n",
    "            if i == 0:\n",
    "                edit_distance = nltk.edit_distance(query, search_range[i])\n",
    "                corrected_queries_by_terms.append(search_range[i])\n",
    "\n",
    "            tmp = nltk.edit_distance(query, search_range[i]) # find most similar for \"correct\" query tokens to terms by using Edit distance / Levenshtein distance\n",
    "\n",
    "            if tmp < edit_distance:\n",
    "                edit_distance = tmp\n",
    "\n",
    "                if corrected_queries_by_terms:\n",
    "                    corrected_queries_by_terms.pop()\n",
    "\n",
    "                corrected_queries_by_terms.append(search_range[i]) # append most similar token into corrected_queries\n",
    "\n",
    "    pb1.update(1) # update progress bar\n",
    "    pb1.set_description(f'Search and preprocess possible terms for query token <{query}>: ', refresh=True)\n",
    "\n",
    "\n",
    "# get all query tokens doc id\n",
    "for item in corrected_queries_by_terms:\n",
    "    term_docs_weights = term_weight_doc_id_dict[item]\n",
    "    all_docs_ids_for_tokenized_queries.update(set(term_docs_weights.keys()))\n",
    "\n",
    "\n",
    "# create dict with all involved doc ids, like {'13191': [], '13185': []}\n",
    "for doc_id in all_docs_ids_for_tokenized_queries:\n",
    "    term_weight_for_tokenized_queries.update({doc_id: []})\n",
    "\n",
    "print(f\"corrected_queries_by_terms: {corrected_queries_by_terms}\")\n",
    "\n",
    "# can output {'13185': [8.79, 0.0, 0.0], '13191': [27.10, 30.52, 0.0], '13194': [0.0, 0.0, 16.73]}\n",
    "pb2 = tqdm(total=len(corrected_queries_by_terms), nrows=4, position=0, leave=True)\n",
    "\n",
    "for corrected_query in corrected_queries_by_terms:\n",
    "    if corrected_query in term_weight_doc_id_dict:\n",
    "        term_docs_weights = term_weight_doc_id_dict[corrected_query] # {'13185': 3.818654696160069, '13191': 7.26724351604199}\n",
    "\n",
    "        for doc_id, weight in term_docs_weights.items():\n",
    "            term_weight_for_tokenized_queries[doc_id].append(weight)\n",
    "\n",
    "        need_to_0_padding = set(all_docs_ids_for_tokenized_queries) - set(term_docs_weights.keys())\n",
    "\n",
    "        if need_to_0_padding:\n",
    "            for k in need_to_0_padding:\n",
    "                term_weight_for_tokenized_queries[k].append(0.0)\n",
    "\n",
    "    pb2.update(1) # update progress bar\n",
    "    pb2.set_description(f'Produce doc id weight vector for each query token: ', refresh=True)\n",
    "\n",
    "# calculate tokens weight in query\n",
    "for query in queries:\n",
    "    tf = queries.count(query)\n",
    "    q_vector.append(calculate_weight(tf, 1))\n",
    "\n",
    "for doc_id, d_vector in term_weight_for_tokenized_queries.items():\n",
    "    similarity = cosine_similarity(d_vector, q_vector)\n",
    "\n",
    "    search_results.append([doc_id, similarity])\n",
    "search_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print results\n",
    "for r in search_results:\n",
    "    print(r[0], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13184 0.7071067811865476\n",
      "13194 0.7071067811865475\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_j = query 在 doc 的 tf-idf weight 向量\n",
    "# q = query 各字在 query 的 tf-idf weight 向量\n",
    "\n",
    "# Hong Kong Business\n",
    "# d_j w_ij 就是第 i 個 term 在第 j 個 doc 的 weight\n",
    "\n",
    "# d_j 有很多，有 13173 個，每個長度都跟 query 一樣\n",
    "# d_j = (Hong在j個doc的weight, kong在j個doc的weight, business在j個doc的weight)\n",
    "# q = (Hong在query內的weight, kong在query內的weight, business在query內的weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73a4b86a3fdc57664cb77b782e9a5af8dc1e3010b538664a98e9fbbcbf98b0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
