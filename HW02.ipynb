{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFormat: term_i, df_i:\\n    <doc1, tf_1: <pos1, pos2, … >; \\n    doc2, tf_2: <pos1, pos2, …>; …>\\n\\nFormat: to, 993427: \\n    <1, 6: <7, 18, 33, 72, 86, 231>;   \\n    2, 5: <1, 17, 74, 222, 255>; … >\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Format: term_i, df_i:\n",
    "    <doc1, tf_1: <pos1, pos2, … >; \n",
    "    doc2, tf_2: <pos1, pos2, …>; …>\n",
    "\n",
    "Format: to, 993427: \n",
    "    <1, 6: <7, 18, 33, 72, 86, 231>;   \n",
    "    2, 5: <1, 17, 74, 222, 255>; … >\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# initialize nltk tokenizer\n",
    "nltk.download('punkt')\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = eval(open('all_docs_freq_all_tokens_dict.txt','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'13185,1': [0]}, {'13191,8': [841, 848, 850, 948, 955, 969, 980, 982]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index['usernam, 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://mathpretty.com/10661.html\n",
    "\n",
    "NUM_OF_ALL_DOCS = 13173 # number of all documents (processed)\n",
    "\n",
    "def calculate_weight(raw_count_in_doc, term_doc_in_all_document):\n",
    "    \"\"\"\n",
    "    raw_count_in_doc : appear times of term in the doc (raw count)\n",
    "    term_doc_in_all_document : number of docs which contains the term (processed)\n",
    "    \"\"\"\n",
    "    idf = NUM_OF_ALL_DOCS / term_doc_in_all_document\n",
    "    w_i_j = (1 + math.log10(raw_count_in_doc)) * math.log10(idf)\n",
    "\n",
    "    return w_i_j\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    assert len(vec_a) == len(vec_b), \"Vectors needs to be same length\"\n",
    "\n",
    "    dot = sum(i * j for i, j in zip(vec_a, vec_b))\n",
    "    a_norm = sum(i**2 for i in vec_a) ** 0.5\n",
    "    b_norm = sum(i**2 for i in vec_b) ** 0.5\n",
    "\n",
    "    cos_sim = dot / (a_norm * b_norm)\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate weight in term_weight_doc_id_dict: : 100%|█████████▉| 168805/168806 [01:38<00:00, 1715.57it/s]"
     ]
    }
   ],
   "source": [
    "term_doc_in_all_document = 0\n",
    "\n",
    "keys = list(inverted_index.keys())\n",
    "d_j = []\n",
    "q = []\n",
    "term_weight_doc_id_dict = dict()\n",
    "\n",
    "pb = tqdm(total=len(keys), nrows=4, position=0, leave=True)\n",
    "\n",
    "for term_df_key in keys:\n",
    "    d_j = []\n",
    "    term = re.findall('\\w+', term_df_key)[0] # 'usernam, 2' -> 'usernam'\n",
    "    term_doc_in_all_document = int(re.findall(', ([0-9]+)', term_df_key)[0]) # 'usernam, 2' -> 2\n",
    "\n",
    "    temp_dict = dict()\n",
    "    for i in range(len(inverted_index[term_df_key])):\n",
    "        doc_raw_count_and_posting = list(inverted_index[term_df_key][i].keys())[0] # [{'13185,1': [0]}, {'13191,8': ...}...]\n",
    "\n",
    "        doc_id = re.findall('([0-9]+),', doc_raw_count_and_posting)[0] # '13185,1' -> 13185 #doc的編號\n",
    "        raw_count_in_doc = int(re.findall(',([0-9]+)', doc_raw_count_and_posting)[0]) # '13185,1' -> 1 #在這個doc出現幾次\n",
    "\n",
    "        weight_i_j = calculate_weight(raw_count_in_doc, term_doc_in_all_document)\n",
    "\n",
    "        temp_dict[doc_id]=weight_i_j\n",
    "\n",
    "    pb.set_description('Calculate weight in term_weight_doc_id_dict: ', refresh=True)\n",
    "    pb.update(1) # update progress bar\n",
    "\n",
    "    term_weight_doc_id_dict[term] = temp_dict\n",
    "\n",
    "    # term_weight_doc_id_dict = {'usernam':{'13185': 8.79, '13191': 27.10}, 'password':{'13191': 30.52}, 'becom': {'13194': 16.73}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'usernam, 2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2d30837dcf7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorrected_queries_by_terms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mterm_docs_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterm_weight_doc_id_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mall_docs_ids_for_tokenized_queries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm_docs_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get all query tokens doc id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'usernam, 2'"
     ]
    }
   ],
   "source": [
    "queries = ('user name', 'passwordsss', 'became')\n",
    "# terms = ('usernam', 'password', 'becom')\n",
    "# queries = ('usernam', 'password', 'becom')\n",
    "# terms = ('usernam', 'password', 'becom')\n",
    "\n",
    "corrected_queries_by_terms = []\n",
    "\n",
    "# term_weight_doc_id_dict = {'usernam':{'13185': 8.79, '13191': 27.10}, 'password':{'13191': 30.52}, 'becom': {'13194': 16.73}}\n",
    "\n",
    "# d_1, d_2, ..., d_j, {'13185': (8.79, 0.00), ...}\n",
    "term_weight_for_tokenized_queries = {}\n",
    "q = []\n",
    "\n",
    "all_docs_ids_for_tokenized_queries = set()\n",
    "\n",
    "for query in queries:\n",
    "    query = stemmer.stem(query) # stemmize the query token\n",
    "\n",
    "    if query in term_weight_doc_id_dict:\n",
    "        corrected_queries_by_terms.append(query)\n",
    "    else:\n",
    "        # get most similar terms for user's query\n",
    "        search_range = [t for t in terms if t[0] == query[0]] # optimize: reduce the search range by starts with same character\n",
    "        # find most similar for \"correct\" query tokens to terms by using Edit distance / Levenshtein distance\n",
    "        for i in range(len(terms)):\n",
    "            if i == 0:\n",
    "                edit_distance = nltk.edit_distance(query, terms[i])\n",
    "                corrected_queries_by_terms.append(terms[i])\n",
    "\n",
    "            tmp = nltk.edit_distance(query, terms[i])\n",
    "\n",
    "            if tmp < edit_distance:\n",
    "                edit_distance = tmp\n",
    "\n",
    "                if corrected_queries_by_terms:\n",
    "                    corrected_queries_by_terms.pop()\n",
    "\n",
    "                corrected_queries_by_terms.append(terms[i]) # append most similar token into corrected_queries\n",
    "\n",
    "\n",
    "for item in corrected_queries_by_terms:\n",
    "    term_docs_weights = term_weight_doc_id_dict[item]\n",
    "    all_docs_ids_for_tokenized_queries.update(set(term_docs_weights.keys())) # get all query tokens doc id\n",
    "\n",
    "for doc_id in all_docs_ids_for_tokenized_queries:\n",
    "    term_weight_for_tokenized_queries.update({doc_id: []}) # create dict with all involved doc ids, like {'13191': [], '13185': []}\n",
    "\n",
    "print(f\"corrected_queries_by_terms: {corrected_queries_by_terms}\")\n",
    "print(f\"all_docs_ids_for_tokenized_queries: {all_docs_ids_for_tokenized_queries}\")\n",
    "print(f\"term_weight_for_tokenized_queries: {term_weight_for_tokenized_queries}\")\n",
    "\n",
    "# 可以輸出 {'13185': [8.79, 0.0, 0.0], '13191': [27.10, 30.52, 0.0], '13194': [0.0, 0.0, 16.73]} \n",
    "for corrected_query in corrected_queries_by_terms:\n",
    "    if corrected_query in term_weight_doc_id_dict:\n",
    "        term_docs_weights = term_weight_doc_id_dict[corrected_query] # {'13191': 30.52}\n",
    "\n",
    "        for doc_id, weight in term_docs_weights.items(): # for doc_id, weight in {'13185': 3.818654696160069, '13191': 7.26724351604199}\n",
    "            # doc_id = str(doc_id) doc_id本來就是str\n",
    "            term_weight_for_tokenized_queries[doc_id].append(weight)\n",
    "        \n",
    "\n",
    "\n",
    "        need_to_0_padding = set(all_docs_ids_for_tokenized_queries) - set(term_docs_weights.keys())\n",
    "\n",
    "        if need_to_0_padding:\n",
    "            for k in need_to_0_padding:\n",
    "                term_weight_for_tokenized_queries[k].append(0.0)\n",
    "\n",
    "all_docs_ids_for_tokenized_queries\n",
    "term_weight_for_tokenized_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.11968469182405, 4.11968469182405, 4.11968469182405]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = ('usernam', 'password', 'becom')\n",
    "\n",
    "q_vector = []\n",
    "\n",
    "for query in queries:\n",
    "    tf = queries.count(query)\n",
    "    q_vector.append(calculate_weight(tf,1))\n",
    "\n",
    "q_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['13191', 0.8150621341757105],\n",
       " ['13185', 0.5773502691896258],\n",
       " ['13194', 0.5773502691896257]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = []\n",
    "for doc_id, d_vector in term_weight_for_tokenized_queries.items():\n",
    "    similarity = cosine_similarity(d_vector,q_vector)\n",
    "\n",
    "    search_results.append([doc_id,similarity])\n",
    "search_results.sort(key=lambda x:x[1],reverse=True)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "# term = \"usernam\"\n",
    "# query = \"username\"\n",
    "\n",
    "# query -> stemmize -> search key (use cosine similarity to get most similiar key) -> get weight\n",
    "\n",
    "# d_j = query 在 doc 的 tf-idf weight 向量\n",
    "# q = query 各字在 query 的 tf-idf weight 向量\n",
    "\n",
    "# Hong Kong Business\n",
    "# d_j w_ij 就是第 i 個 term 在第 j 個 doc 的 weight\n",
    "\n",
    "# d_j 有很多，有 13173 個，每個長度都跟 query 一樣\n",
    "# d_j = (Hong在j個doc的weight, kong在j個doc的weight, business在j個doc的weight)\n",
    "# q = (Hong在query內的weight, kong在query內的weight, business在query內的weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
