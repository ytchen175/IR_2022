{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format: term_i, df_i:\n",
    "    <doc1, tf_1: <pos1, pos2, … >; \n",
    "    doc2, tf_2: <pos1, pos2, …>; …>\n",
    "\n",
    "Format: to, 993427: \n",
    "    <1, 6: <7, 18, 33, 72, 86, 231>;   \n",
    "    2, 5: <1, 17, 74, 222, 255>; … >\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wirl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# initialize nltk tokenizer\n",
    "nltk.download('punkt')\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = eval(open('all_docs_freq_all_tokens_dict.txt','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'13185,1': [0]}, {'13191,8': [841, 848, 850, 948, 955, 969, 980, 982]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index['usernam, 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://mathpretty.com/10661.html\n",
    "term = \"usernam\"\n",
    "\n",
    "NUM_OF_ALL_DOCS = 13173 # number of all documents (processed)\n",
    "\n",
    "def calculate_weight(raw_count_in_doc, term_doc_in_all_document):\n",
    "    \"\"\"\n",
    "    raw_count_in_doc : appear times of term in the doc (raw count)\n",
    "    term_doc_in_all_document : number of docs which contains the term (processed)\n",
    "    \"\"\"\n",
    "    idf = NUM_OF_ALL_DOCS / term_doc_in_all_document\n",
    "    w_i_j = (1 + math.log10(raw_count_in_doc)) * math.log10(idf)\n",
    "\n",
    "    return w_i_j\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    assert len(vec_a) == len(vec_b), \"Vectors needs to be same length\"\n",
    "\n",
    "    dot = sum(i * j for i, j in zip(vec_a, vec_b))\n",
    "    a_norm = sum(i**2 for i in vec_a) ** 0.5\n",
    "    b_norm = sum(i**2 for i in vec_b) ** 0.5\n",
    "\n",
    "    cos_sim = dot / (a_norm * b_norm)\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usernam 13191 [8.792777378669882, 27.076843926618768]\n",
      "member 13193 [30.52191019395961, 8.387312270561717, 8.387312270561717]\n"
     ]
    }
   ],
   "source": [
    "term_doc_in_all_document = 0\n",
    "\n",
    "keys = ['usernam, 2', 'member, 3'] # for test\n",
    "# keys = ['usernam, 2'] # for test\n",
    "d_j = []\n",
    "q = []\n",
    "term_weight_doc_id_dict = dict()\n",
    "# for term_df_key in list(inverted_index.keys()):\n",
    "for term_df_key in list(keys):\n",
    "    d_j = []\n",
    "    term = re.findall('\\w+', term_df_key)[0] # 'usernam, 2' -> 'usernam'\n",
    "    term_doc_in_all_document = int(re.findall(', ([0-9]+)', term_df_key)[0]) # 'usernam, 2' -> 2\n",
    "\n",
    "    # print(term_df_key)\n",
    "    # print(inverted_index[term_df_key])\n",
    "    \n",
    "    temp_dict = dict()\n",
    "    for i in range(len(inverted_index[term_df_key])):\n",
    "        doc_raw_count_and_posting = list(inverted_index[term_df_key][i].keys())[0] # [{'13185,1': [0]}, {'13191,8': ...}...]\n",
    "        print(doc_raw_count_and_posting)\n",
    "        doc_id = re.findall('([0-9]+),', doc_raw_count_and_posting)[0] # '13185,1' -> 13185 #doc的編號\n",
    "        raw_count_in_doc = int(re.findall(',([0-9]+)', doc_raw_count_and_posting)[0]) # '13185,1' -> 1 #在這個doc出現幾次\n",
    "        weight_i_j = calculate_weight(raw_count_in_doc, term_doc_in_all_document)\n",
    "        temp_dict[doc_id]=weight_i_j\n",
    "    \n",
    "\n",
    "    term_weight_doc_id_dict[term] = temp_dict\n",
    "\n",
    "print(term_weight_doc_id_dict)\n",
    "    # print(term, doc_id, d_j)\n",
    "    # term_weight_doc_id_dict = {'usernam':{'13185': 8.79, '13191': 27.10}, 'password':{'13191': 30.52}, 'becom': {'13194': 16.73}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrected_queries_by_terms: ['usernam', 'password', 'becom']\n",
      "all_docs_ids_for_tokenized_queries: set()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'13185'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wirl\\Documents\\111-1_Fall_master_1\\IR\\IR_2022\\HW02.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wirl/Documents/111-1_Fall_master_1/IR/IR_2022/HW02.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc_id, weight \u001b[39min\u001b[39;00m term_docs_weights\u001b[39m.\u001b[39mitems(): \u001b[39m# 13185, 13191\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wirl/Documents/111-1_Fall_master_1/IR/IR_2022/HW02.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     doc_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(doc_id)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/wirl/Documents/111-1_Fall_master_1/IR/IR_2022/HW02.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     term_weight_for_tokenized_queries[doc_id]\u001b[39m.\u001b[39mappend(weight)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wirl/Documents/111-1_Fall_master_1/IR/IR_2022/HW02.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m need_to_0_padding \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(all_docs_ids_for_tokenized_queries) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(term_docs_weights\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/wirl/Documents/111-1_Fall_master_1/IR/IR_2022/HW02.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mif\u001b[39;00m need_to_0_padding:\n",
      "\u001b[1;31mKeyError\u001b[0m: '13185'"
     ]
    }
   ],
   "source": [
    "queries = ('user name', 'passwordsss', 'became')\n",
    "terms = ('usernam', 'password', 'becom')\n",
    "\n",
    "corrected_queries_by_terms = []\n",
    "\n",
    "term_weight_doc_id_dict = {'usernam':{'13185': 8.79, '13191': 27.10}, 'password':{'13191': 30.52}, 'becom': {'13194': 16.73}}\n",
    "\n",
    "# d_1, d_2, ..., d_j, {'13185': (8.79, 0.00), ...}\n",
    "term_weight_for_tokenized_queries = {}\n",
    "q = []\n",
    "\n",
    "all_docs_ids_for_tokenized_queries = set()\n",
    "\n",
    "for query in queries:\n",
    "    query = stemmer.stem(query) # stemmize the query token\n",
    "\n",
    "    if query in term_weight_doc_id_dict:\n",
    "        term_docs_weights = term_weight_doc_id_dict[query]\n",
    "        all_docs_ids_for_tokenized_queries = all_docs_ids_for_tokenized_queries.union(set(term_docs_weights.keys())) # get all query tokens doc id\n",
    "\n",
    "        for doc_id in all_docs_ids_for_tokenized_queries:\n",
    "            term_weight_for_tokenized_queries.update({doc_id: []}) # create dict with all involved doc ids, like {'13191': [], '13185': []}\n",
    "    else:\n",
    "        # get most similar terms for user's query\n",
    "        search_range = [t for t in terms if t[0] == query[0]] # optimize: reduce the search range by starts with same character\n",
    "\n",
    "        # find most similar for \"correct\" query tokens to terms by using Edit distance / Levenshtein distance\n",
    "        for i in range(len(terms)):\n",
    "            if i == 0:\n",
    "                edit_distance = nltk.edit_distance(query, terms[i])\n",
    "                corrected_queries_by_terms.append(terms[i])\n",
    "\n",
    "            tmp = nltk.edit_distance(query, terms[i])\n",
    "\n",
    "            if tmp < edit_distance:\n",
    "                edit_distance = tmp\n",
    "\n",
    "                if corrected_queries_by_terms:\n",
    "                    corrected_queries_by_terms.pop()\n",
    "\n",
    "                corrected_queries_by_terms.append(terms[i]) # append most similar token into corrected_queries\n",
    "\n",
    "print(f\"corrected_queries_by_terms: {corrected_queries_by_terms}\")\n",
    "print(f\"all_docs_ids_for_tokenized_queries: {all_docs_ids_for_tokenized_queries}\")\n",
    "\n",
    "# TODO: 把這個修好，原本可以輸出 {'13185': [8.79, 0.0, 0.0], '13191': [27.10, 30.52, 0.0], '13194': [0.0, 0.0, 16.73]} 的但 correct 完 queries 就不行了\n",
    "for corrected_query in corrected_queries_by_terms:\n",
    "    if corrected_query in term_weight_doc_id_dict:\n",
    "        term_docs_weights = term_weight_doc_id_dict[corrected_query] # {'13191': 30.52}\n",
    "\n",
    "        for doc_id, weight in term_docs_weights.items(): # 13185, 13191\n",
    "            doc_id = str(doc_id)\n",
    "            term_weight_for_tokenized_queries[doc_id].append(weight)\n",
    "\n",
    "        need_to_0_padding = set(all_docs_ids_for_tokenized_queries) - set(term_docs_weights.keys())\n",
    "\n",
    "        if need_to_0_padding:\n",
    "            for k in need_to_0_padding:\n",
    "                term_weight_for_tokenized_queries[k].append(0.0)\n",
    "\n",
    "all_docs_ids_for_tokenized_queries\n",
    "term_weight_for_tokenized_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "# term = \"usernam\"\n",
    "# query = \"username\"\n",
    "\n",
    "# query -> stemmize -> search key (use cosine similarity to get most similiar key) -> get weight\n",
    "\n",
    "# d_j = query 在 doc 的 tf-idf weight 向量\n",
    "# q = query 各字在 query 的 tf-idf weight 向量\n",
    "\n",
    "# Hong Kong Business\n",
    "# d_j w_ij 就是第 i 個 term 在第 j 個 doc 的 weight\n",
    "\n",
    "# d_j 有很多，有 13173 個，每個長度都跟 query 一樣\n",
    "# d_j = (Hong在j個doc的weight, kong在j個doc的weight, business在j個doc的weight)\n",
    "# q = (Hong在query內的weight, kong在query內的weight, business在query內的weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73a4b86a3fdc57664cb77b782e9a5af8dc1e3010b538664a98e9fbbcbf98b0cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
