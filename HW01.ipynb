{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DOCS_PATH = 'docs'\n",
    "PROCESSED_DOCS_PATH = 'processed_docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split warc file to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref : https://stackoverflow.com/questions/60269904/split-text-file-after-specific-line-in-python\n",
    "SECTION_START = re.compile(r'<!DOCTYPE html')\n",
    "SECTION_END = re.compile(r'</html>')\n",
    "\n",
    "def split_docs_iter(stream):\n",
    "    def inner(stream):\n",
    "        # Yields each line until an end marker is found (or EOF)\n",
    "        for line in stream:\n",
    "            if line and not SECTION_END.match(line):\n",
    "                yield line\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # Find a start marker, then break off into a nested iterator\n",
    "    for line in stream:\n",
    "        if line:\n",
    "            if SECTION_START.match(line):\n",
    "                yield inner(stream)\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"03.warc\"\n",
    "\n",
    "# split docs\n",
    "with open(filename, 'r', encoding=\"ISO-8859-1\") as fh_in:\n",
    "    for (i, nested_iter) in enumerate(split_docs_iter(fh_in)):\n",
    "        with open('./docs/docID_{:05d}'.format(i), 'w', encoding='UTF-8') as fh_out:\n",
    "            for line in nested_iter:\n",
    "                fh_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse html and get text in <body> tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID_01864 don't have body\n",
      "docID_01865 don't have body\n",
      "docID_01866 don't have body\n",
      "docID_01867 don't have body\n",
      "docID_01868 don't have body\n",
      "docID_01869 don't have body\n",
      "docID_01870 don't have body\n",
      "docID_01871 don't have body\n",
      "docID_01872 don't have body\n",
      "docID_02492 don't have body\n",
      "docID_05721 don't have body\n",
      "docID_05722 don't have body\n",
      "docID_05723 don't have body\n",
      "docID_05724 don't have body\n",
      "docID_05725 don't have body\n",
      "docID_05726 don't have body\n",
      "docID_05727 don't have body\n",
      "docID_05728 don't have body\n",
      "docID_05729 don't have body\n",
      "docID_05792 don't have body\n",
      "docID_05842 don't have body\n",
      "docID_10104 don't have body\n"
     ]
    }
   ],
   "source": [
    "files = [f for f in listdir(DOCS_PATH) if isfile(join(DOCS_PATH, f))]\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(f\"{DOCS_PATH}/{file}\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "            # get text in <body>\n",
    "            body_text = soup.find('body').getText()\n",
    "            # Remove newline characters, Home\\nHi -> Home Hi\n",
    "            concatenated_body_text = \" \".join(body_text.split())\n",
    "            # Case folding, A -> a, additional character -> \"\"\n",
    "            processed_concatenated_body_text = re.sub(r\"[^A-Za-z0-9]+\", ' ', concatenated_body_text).lower()\n",
    "\n",
    "            with open(f\"{PROCESSED_DOCS_PATH}/{file}_processed\", mode=\"w\", encoding=\"utf-8\", errors='strict', buffering=1) as f1:\n",
    "                f1.write(processed_concatenated_body_text)\n",
    "    # skip the docs which not have <body>\n",
    "    except (OSError, AttributeError) as e:\n",
    "        print(f\"{file} don't have body\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\T160\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\T160\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# initialize nltk tokenizer\n",
    "nltk.download('punkt')\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files preprocessed needed: : 100%|██████████| 13173/13173 [1:01:14<00:00,  3.58it/s]\n",
      "Transforming output format: : 100%|██████████| 168806/168806 [02:55<00:00, 959.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_docs_freq_all_tokens_dict successfully saved !\n"
     ]
    }
   ],
   "source": [
    "# all docs with all tokens dict\n",
    "all_docs_all_tokens_dict = {}\n",
    "# final key name\n",
    "each_term_in_each_doc_freq_dict = {}\n",
    "\n",
    "processed_files = [f for f in listdir(PROCESSED_DOCS_PATH) if isfile(join(PROCESSED_DOCS_PATH, f))]\n",
    "# processed_files = ['docID_00000_processed', 'docID_00001_processed'] # for test\n",
    "\n",
    "# initialize the progress bar\n",
    "p = tqdm(total=len(processed_files), nrows=4, position=0, leave=True)\n",
    "\n",
    "# traverse processed\n",
    "for p_file in processed_files:\n",
    "    with open(f\"{PROCESSED_DOCS_PATH}/{p_file}\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        contents = f.read()\n",
    "\n",
    "        doc_num = re.findall(r\"[0-9]+\", p_file)[0]\n",
    "        single_doc_tokens_dict = {}\n",
    "        doc_freq = 0\n",
    "\n",
    "        tokenized = word_tokenizer.tokenize(contents)\n",
    "        # tokenized = [\"usernam\", \"member\", \"usernam\"] # for test\n",
    "\n",
    "        tokenized_and_rm_stopwords_and_stemmed = [stemmer.stem(word) for word in tokenized if word not in stopwords.words('english') and not stemmer.stem(word).isnumeric()]\n",
    "        tokens_dict = Counter(tokenized_and_rm_stopwords_and_stemmed)\n",
    "\n",
    "        distinct_tokens = tokens_dict.keys()\n",
    "\n",
    "        for term in distinct_tokens:\n",
    "            pos_list = [i for i, x in enumerate(tokenized_and_rm_stopwords_and_stemmed) if x == term]\n",
    "            term_freq = len(pos_list)\n",
    "            inner_key_format = f\"{doc_num},{term_freq}\"\n",
    "\n",
    "            # doc level\n",
    "            # output format\n",
    "            # d1 = {\n",
    "            #  'salaka': [\n",
    "            #     {'00000,2': [4, 35]}\n",
    "            #     ],\n",
    "            #  'time': [\n",
    "            #     {'00000,2': [9, 98]}\n",
    "            #     ]\n",
    "            # }\n",
    "            if term not in single_doc_tokens_dict:\n",
    "                single_doc_tokens_dict[term] = []\n",
    "\n",
    "            single_doc_tokens_dict[term].append(\n",
    "                {inner_key_format: pos_list}\n",
    "            )\n",
    "\n",
    "            # all docs level\n",
    "            # output format\n",
    "            # d1 = {\n",
    "            #  'salaka': [\n",
    "            #     {'00000,2': [4, 35]}, {'00001,1': [36]}\n",
    "            #     ],\n",
    "            #  'time': [\n",
    "            #     {'00000,2': [7, 10]}, {'00001,3': [9, 98, 100]}\n",
    "            #     ]\n",
    "            # }\n",
    "            if term not in all_docs_all_tokens_dict:\n",
    "                all_docs_all_tokens_dict[term] = []\n",
    "                all_docs_all_tokens_dict.update(single_doc_tokens_dict)\n",
    "            else:\n",
    "                all_docs_all_tokens_dict[term].append(\n",
    "                    single_doc_tokens_dict[term][0]\n",
    "                )\n",
    "\n",
    "        p.set_description('Files preprocessed needed: ', refresh=True)\n",
    "        p.update(1) # update progress bar\n",
    "\n",
    "p.close()\n",
    "\n",
    "# initialize the progress bar\n",
    "pb = tqdm(total=len(all_docs_all_tokens_dict), nrows=4, position=0, leave=True)\n",
    "\n",
    "# prepare a dict like is: {'usernam': 'usernam, 4', 'member': 'member, 2'}\n",
    "for term, doc_and_doc_freq_pos_list in all_docs_all_tokens_dict.items():\n",
    "    term_in_doc_freq = 0\n",
    "\n",
    "    for doc_and_doc_freq in doc_and_doc_freq_pos_list:\n",
    "        each_doc_key_freq = int(list(doc_and_doc_freq.keys())[0].split(',')[1])\n",
    "        term_in_doc_freq += each_doc_key_freq\n",
    "\n",
    "    each_term_in_each_doc_freq_dict[term] = f'{term}, {term_in_doc_freq}'\n",
    "\n",
    "    pb.set_description('Transforming output format: ', refresh=True)\n",
    "    pb.update(1) # update progress bar\n",
    "\n",
    "pb.close()\n",
    "\n",
    "# update outer dict key count\n",
    "all_docs_freq_all_tokens_dict = dict((each_term_in_each_doc_freq_dict[key], value) for (key, value) in all_docs_all_tokens_dict.items())\n",
    "\n",
    "# save as file\n",
    "with open('all_docs_freq_all_tokens_dict.txt','w') as t:\n",
    "    t.write(str(all_docs_freq_all_tokens_dict))\n",
    "\n",
    "print(\"all_docs_freq_all_tokens_dict successfully saved !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format: to, 993427: \n",
    "    <1, 6: <7, 18, 33, 72, 86, 231>;   \n",
    "    2, 5: <1, 17, 74, 222, 255>; … >\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cb0fd0868c59445a8ea216444055d94ae2f6ad590b5739d464df9e6b892303c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
