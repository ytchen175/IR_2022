{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split warc file to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref : https://stackoverflow.com/questions/60269904/split-text-file-after-specific-line-in-python\n",
    "SECTION_START = re.compile(r'<!DOCTYPE html')\n",
    "SECTION_END = re.compile(r'</html>')\n",
    "\n",
    "def split_docs_iter(stream):\n",
    "    def inner(stream):\n",
    "        # Yields each line until an end marker is found (or EOF)\n",
    "        for line in stream:\n",
    "            if line and not SECTION_END.match(line):\n",
    "                yield line\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # Find a start marker, then break off into a nested iterator\n",
    "    for line in stream:\n",
    "        if line:\n",
    "            if SECTION_START.match(line):\n",
    "                yield inner(stream)\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"03.warc\"\n",
    "\n",
    "# split docs\n",
    "with open(filename, 'r', encoding=\"ISO-8859-1\") as fh_in:\n",
    "    for (i, nested_iter) in enumerate(split_docs_iter(fh_in)):\n",
    "        with open('./docs/docID_{:05d}'.format(i), 'w', encoding='UTF-8') as fh_out:\n",
    "            for line in nested_iter:\n",
    "                fh_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse html and get text in <body> tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID_01864 don't have body\n",
      "docID_01865 don't have body\n",
      "docID_01866 don't have body\n",
      "docID_01867 don't have body\n",
      "docID_01868 don't have body\n",
      "docID_01869 don't have body\n",
      "docID_01870 don't have body\n",
      "docID_01871 don't have body\n",
      "docID_01872 don't have body\n",
      "docID_02492 don't have body\n",
      "docID_05721 don't have body\n",
      "docID_05722 don't have body\n",
      "docID_05723 don't have body\n",
      "docID_05724 don't have body\n",
      "docID_05725 don't have body\n",
      "docID_05726 don't have body\n",
      "docID_05727 don't have body\n",
      "docID_05728 don't have body\n",
      "docID_05729 don't have body\n",
      "docID_05792 don't have body\n",
      "docID_05842 don't have body\n",
      "docID_10104 don't have body\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "DOCS_PATH = 'docs'\n",
    "PROCESSED_DOCS_PATH = 'processed_docs'\n",
    "\n",
    "files = [f for f in listdir(DOCS_PATH) if isfile(join(DOCS_PATH, f))]\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(f\"{DOCS_PATH}/{file}\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "            # get text in <body>\n",
    "            body_text = soup.find('body').getText()\n",
    "            # Remove newline characters, Home\\nHi -> Home Hi\n",
    "            concatenated_body_text = \" \".join(body_text.split())\n",
    "            # Case folding, A -> a, additional character -> \"\"\n",
    "            processed_concatenated_body_text = re.sub(r\"[^A-Za-z0-9]+\", ' ', concatenated_body_text).lower()\n",
    "\n",
    "            with open(f\"{PROCESSED_DOCS_PATH}/{file}_processed\", mode=\"w\", encoding=\"utf-8\", errors='strict', buffering=1) as f1:\n",
    "                f1.write(processed_concatenated_body_text)\n",
    "    # skip the docs which not have <body>\n",
    "    except (OSError, AttributeError) as e:\n",
    "        print(f\"{file} don't have body\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\T160\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\T160\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# initialize nltk tokenizer\n",
    "nltk.download('punkt')\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'usernam': [{'00000,2': [0, 2]}, [{'00001,2': [0, 2]}]],\n",
       " 'member': [{'00000,1': [1]}, [{'00001,1': [1]}]]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all docs with all tokens dict\n",
    "all_docs_all_tokens_dict = {}\n",
    "# final key name\n",
    "each_term_in_each_doc_freq_dict = {}\n",
    "\n",
    "# processed_files = [f for f in listdir(PROCESSED_DOCS_PATH) if isfile(join(PROCESSED_DOCS_PATH, f))]\n",
    "\n",
    "processed_files = ['docID_00000_processed', 'docID_00001_processed']\n",
    "\n",
    "\n",
    "for p_file in processed_files:\n",
    "    with open(f\"{PROCESSED_DOCS_PATH}/{p_file}\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        contents = f.read()\n",
    "\n",
    "        doc_num = re.findall(r\"[0-9]+\", p_file)[0]\n",
    "        single_doc_tokens_dict = {}\n",
    "        doc_freq = 0\n",
    "\n",
    "        # tokenized = word_tokenizer.tokenize(contents)\n",
    "        tokenized = [\"usernam\", \"member\", \"usernam\"]\n",
    "\n",
    "        tokenized_and_rm_stopwords_and_stemmed = [stemmer.stem(word) for word in tokenized if word not in stopwords.words('english') and not stemmer.stem(word).isnumeric()]\n",
    "        tokens_dict = Counter(tokenized_and_rm_stopwords_and_stemmed)\n",
    "\n",
    "        distinct_tokens = tokens_dict.keys()\n",
    "\n",
    "        for term in distinct_tokens:\n",
    "            pos_list = [i for i, x in enumerate(tokenized_and_rm_stopwords_and_stemmed) if x == term]\n",
    "            term_freq = len(pos_list)\n",
    "            inner_key_format = f\"{doc_num},{term_freq}\"\n",
    "\n",
    "            # doc level\n",
    "            # output format\n",
    "            # d1 = {\n",
    "            #  'salaka': [\n",
    "            #     {'00000,2': [4, 35]}\n",
    "            #     ],\n",
    "            #  'time': [\n",
    "            #     {'00000,2': [9, 98]}\n",
    "            #     ]\n",
    "            # }\n",
    "            if term not in single_doc_tokens_dict:\n",
    "                single_doc_tokens_dict[term] = []\n",
    "\n",
    "            single_doc_tokens_dict[term].append({inner_key_format: pos_list})\n",
    "\n",
    "            # all docs level\n",
    "            # output format\n",
    "            # d1 = {\n",
    "            #  'salaka': [\n",
    "            #     {'00000,2': [4, 35]}, {'00001,1': [36]}\n",
    "            #     ],\n",
    "            #  'time': [\n",
    "            #     {'00000,2': [7, 10]}, {'00001,3': [9, 98, 100]}\n",
    "            #     ]\n",
    "            # }\n",
    "            if term not in all_docs_all_tokens_dict:\n",
    "                all_docs_all_tokens_dict[term] = []\n",
    "                all_docs_all_tokens_dict.update(single_doc_tokens_dict)\n",
    "            else:\n",
    "                all_docs_all_tokens_dict[term].append(single_doc_tokens_dict[term])\n",
    "\n",
    "all_docs_all_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usernam\n",
      "[{'00000,2': [0, 2]}, [{'00001,2': [0, 2]}]]\n",
      "member\n",
      "[{'00000,1': [1]}, [{'00001,1': [1]}]]\n"
     ]
    }
   ],
   "source": [
    "# in multi doc\n",
    "# total_term_doc_freq_dict = {'alprazolam': 2, 'onlin': 1}\n",
    "# term_in_each_doc_freq_dict = {'alprazolam': \"alprazolam,5\", 'onlin': \"onlin,4\"}\n",
    "\n",
    "# dict((term_in_each_doc_freq_dict[key], value) for (key, value) in total_term_doc_freq_dict.items())\n",
    "\n",
    "total_term_doc_freq_dict = {'alprazolam': 2, 'onlin': 1}\n",
    "term_in_each_doc_freq_dict = {'alprazolam': \"alprazolam,5\", 'onlin': \"onlin,4\"}\n",
    "\n",
    "# 每次 iteration 做的是:\n",
    "# 找 term 並 append {'2, 3': [4, 7, 10]} 進去，沒有的話新增一個\n",
    "# 更新外層 dict 的 key 中的 count\n",
    "# d1 = {\n",
    "#     'alprazolam, 5':{\n",
    "#         {'1, 2': [0, 4]}, {'2, 3': [4, 7, 10]}\n",
    "#     },\n",
    "#     'onlin, 4':{\n",
    "#         {'1, 3': [7, 10, 11]}, {'2, 1': [4]}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # final key name\n",
    "each_term_in_each_doc_freq_dict = {}\n",
    "\n",
    "# d1 = {\n",
    "#     'alprazolam':[\n",
    "#         {'1,2': [0, 4]}, {'2,3': [4, 7, 10]}\n",
    "#     ],\n",
    "#     'onlin':[\n",
    "#         {'1,3': [7, 10, 11]}, {'2,1': [4]}\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "\n",
    "for term, doc_and_doc_freq_pos_list in all_docs_all_tokens_dict.items():\n",
    "    term_in_doc_freq = 0\n",
    "    print(term)\n",
    "    print(doc_and_doc_freq_pos_list)\n",
    "\n",
    "    for doc_and_doc_freq in doc_and_doc_freq_pos_list:\n",
    "        each_doc_key_freq = int(doc_and_doc_freq.split(',')[1])\n",
    "        term_in_doc_freq += each_doc_key_freq\n",
    "    \n",
    "# TODO : 要準備 total_term_doc_freq_dict 跟 term_in_each_doc_freq_dict\n",
    "\n",
    "# # 最後再一次跟total_term_doc_freq_dict換key name\n",
    "# each_term_in_each_doc_freq_dict.update({term: f\"{term},{term_in_doc_freq}\"})\n",
    "\n",
    "# each_term_in_each_doc_freq_dict\n",
    "# d1.keys()\n",
    "# \n",
    "# print(total_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{PROCESSED_DOCS_PATH}/docID_00001_processed\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "#     contents = f.read()\n",
    "#     doc_num = re.findall(r\"[0-9]+\", \"docID_00001_processed\")[0]\n",
    "#     tokenized = word_tokenizer.tokenize(contents)\n",
    "#     tokenized_and_rm_stopwords_and_stemmed = [stemmer.stem(word) for word in tokenized if word not in stopwords.words('english') and not stemmer.stem(word).isnumeric()]\n",
    "#     tokens_dict = Counter(tokenized_and_rm_stopwords_and_stemmed)\n",
    "#     print(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Format: to, 993427: \n",
    "    <1, 6: <7, 18, 33, 72, 86, 231>;   \n",
    "    2, 5: <1, 17, 74, 222, 255>; … >\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'25mg': [{'00001,14': [0, 2, 22, 34, 39, 44, 50, 53, 74, 100, 130, 140, 164, 200]}], 'alprazolam': [{'00001,26': [1, 3, 23, 33, 38, 40, 46, 51, 61, 69, 73, 81, 84, 101, 109, 119, 120, 121, 125, 129, 132, 141, 159, 172, 175, 199]}], 'salaka': [{'00001,2': [4, 35]}], 'babolaz': [{'00001,2': [5, 36]}], 'com': [{'00001,3': [6, 37, 134]}], 'date': [{'00001,1': [7]}], 'aug': [{'00001,1': [8]}], 'time': [{'00001,2': [9, 98]}], 'remot': [{'00001,1': [10]}], 'name': [{'00001,2': [11, 178]}], 'comment': [{'00001,1': [12]}], 'tri': [{'00001,1': [13]}], 'http': [{'00001,1': [14]}], 'acnet': [{'00001,1': [15]}], 'pratt': [{'00001,1': [16]}], 'edu': [{'00001,1': [17]}], 'mcolumbi': [{'00001,1': [18]}], 'order': [{'00001,1': [19]}], 'onlin': [{'00001,12': [20, 111, 176, 186, 202, 212, 216, 229, 238, 244, 249, 298]}], 'ativan': [{'00001,4': [21, 190, 266, 292]}], 'level': [{'00001,1': [24]}], 'discuss': [{'00001,2': [25, 92]}], 'home': [{'00001,1': [26]}], 'content': [{'00001,1': [27]}], 'search': [{'00001,2': [28, 255]}], 'post': [{'00001,4': [29, 96, 261, 263]}], 'repli': [{'00001,1': [30]}], 'next': [{'00001,1': [31]}], 'previou': [{'00001,1': [32]}], 'tablet': [{'00001,7': [41, 47, 52, 110, 124, 128, 131]}], 'usp': [{'00001,2': [42, 48]}], 'civ': [{'00001,2': [43, 49]}], 'ndc': [{'00001,1': [45]}], 'color': [{'00001,1': [54]}], 'white': [{'00001,1': [55]}], 'shape': [{'00001,1': [56]}], 'oval': [{'00001,1': [57]}], 'mark': [{'00001,1': [58]}], 'score': [{'00001,1': [59]}], 'deboss': [{'00001,1': [60]}], 'dava': [{'00001,1': [62]}], 'pharmaceut': [{'00001,1': [63]}], 'inc': [{'00001,1': [64]}], 'gener': [{'00001,5': [65, 71, 75, 272, 282]}], 'drug': [{'00001,6': [66, 112, 116, 184, 215, 293]}], 'canada': [{'00001,1': [67]}], 'extend': [{'00001,1': [68]}], 'buy': [{'00001,9': [70, 173, 227, 237, 242, 274, 278, 295, 299]}], 'fluoxetin': [{'00001,1': [72]}], 'prozac': [{'00001,1': [76]}], 'stop': [{'00001,1': [77]}], 'watch': [{'00001,1': [78]}], 'price': [{'00001,3': [79, 117, 136]}], 'u': [{'00001,1': [80]}], 'cash': [{'00001,1': [82]}], 'deliveri': [{'00001,1': [83]}], 'well': [{'00001,2': [85, 102]}], 'known': [{'00001,1': [86]}], 'medic': [{'00001,2': [87, 206]}], 'review': [{'00001,1': [88]}], 'one': [{'00001,1': [89]}], 'could': [{'00001,1': [90]}], 'fda': [{'00001,1': [91]}], 'factor': [{'00001,1': [93]}], 'beyond': [{'00001,1': [94]}], 'minut': [{'00001,1': [95]}], 'earlier': [{'00001,1': [97]}], 'point': [{'00001,1': [99]}], 'healthi': [{'00001,1': [103]}], 'volunt': [{'00001,1': [104]}], 'rather': [{'00001,1': [105]}], 'biotech': [{'00001,1': [106]}], 'compani': [{'00001,1': [107]}], 'offer': [{'00001,1': [108]}], 'catalog': [{'00001,1': [113]}], 'back': [{'00001,1': [114]}], 'pharmaci': [{'00001,4': [115, 187, 203, 290]}], 'inform': [{'00001,1': [118]}], '2mg': [{'00001,1': [122]}], 'hour': [{'00001,2': [123, 127]}], '3mg': [{'00001,1': [126]}], 'drugstor': [{'00001,1': [133]}], 'low': [{'00001,1': [135]}], 'side': [{'00001,1': [137]}], 'effect': [{'00001,1': [138]}], 'interact': [{'00001,2': [139, 294]}], 'curl': [{'00001,1': [142]}], 'season': [{'00001,1': [143]}], 'scottish': [{'00001,1': [144]}], 'said': [{'00001,1': [145]}], 'half': [{'00001,1': [146]}], 'forgotten': [{'00001,1': [147]}], 'look': [{'00001,1': [148]}], 'thank': [{'00001,1': [149]}], 'guestbook': [{'00001,1': [150]}], 'regi': [{'00001,1': [151]}], 'news': [{'00001,2': [152, 310]}], 'children': [{'00001,1': [153]}], 'breath': [{'00001,1': [154]}], 'slur': [{'00001,1': [155]}], 'speech': [{'00001,1': [156]}], 'consid': [{'00001,1': [157]}], 'take': [{'00001,1': [158]}], 'edit': [{'00001,1': [160]}], 'percocet': [{'00001,1': [161]}], 'oral': [{'00001,2': [162, 165]}], 'solut': [{'00001,1': [163]}], 'panic': [{'00001,1': [166]}], 'disord': [{'00001,1': [167]}], 'impair': [{'00001,1': [168]}], 'liver': [{'00001,1': [169]}], 'diseas': [{'00001,1': [170]}], 'myasthenia': [{'00001,1': [171]}], 'cheap': [{'00001,5': [174, 182, 230, 281, 289]}], 'brand': [{'00001,1': [177]}], 'zoloft': [{'00001,3': [179, 222, 286]}], 'uk': [{'00001,1': [180]}], 'find': [{'00001,1': [181]}], 'qualiti': [{'00001,1': [183]}], 'prescript': [{'00001,8': [185, 201, 214, 217, 231, 239, 247, 301]}], 'xanax': [{'00001,3': [188, 233, 275]}], 'versu': [{'00001,1': [189]}], 'pad': [{'00001,1': [191]}], 'almost': [{'00001,1': [192]}], 'alway': [{'00001,1': [193]}], 'zero': [{'00001,1': [194]}], 'prefix': [{'00001,1': [195]}], 'decim': [{'00001,1': [196]}], 'correct': [{'00001,1': [197]}], 'check': [{'00001,1': [198]}], 'trustworthi': [{'00001,1': [204]}], 'place': [{'00001,1': [205]}], 'portal': [{'00001,1': [207]}], 'recommend': [{'00001,1': [208]}], 'site': [{'00001,1': [209]}], 'need': [{'00001,1': [210]}], 'categori': [{'00001,1': [211]}], 'hydrocodon': [{'00001,2': [213, 223]}], 'vicodin': [{'00001,2': [218, 232]}], 'diet': [{'00001,4': [219, 235, 243, 250]}], 'ephedrin': [{'00001,1': [220]}], 'paxil': [{'00001,2': [221, 288]}], 'snort': [{'00001,1': [224]}], 'meridia': [{'00001,1': [225]}], 'sale': [{'00001,1': [226]}], 'carisoprodol': [{'00001,1': [228]}], 'acomplia': [{'00001,1': [234]}], 'pill': [{'00001,3': [236, 246, 251]}], 'vaniqa': [{'00001,1': [240]}], 'viagra': [{'00001,4': [241, 283, 291, 297]}], 'phentermin': [{'00001,3': [245, 248, 300]}], 'top': [{'00001,1': [252]}], 'link': [{'00001,1': [253]}], 'yahoo': [{'00001,1': [254]}], 'usa': [{'00001,1': [256]}], 'today': [{'00001,1': [257]}], 'youtub': [{'00001,1': [258]}], 'new': [{'00001,1': [259]}], 'york': [{'00001,1': [260]}], 'washington': [{'00001,1': [262]}], 'alexa': [{'00001,1': [264]}], 'archiv': [{'00001,1': [265]}], 'data': [{'00001,1': [267]}], 'ambien': [{'00001,1': [268]}], 'discount': [{'00001,2': [269, 296]}], 'clonazepam': [{'00001,1': [270]}], 'overnight': [{'00001,2': [271, 276]}], 'lorazepam': [{'00001,1': [273]}], 'ship': [{'00001,1': [277]}], 'fioricet': [{'00001,1': [279]}], 'rx': [{'00001,1': [280]}], 'nexium': [{'00001,1': [284]}], 'counter': [{'00001,1': [285]}], 'vs': [{'00001,1': [287]}], 'atim': [{'00001,1': [302]}], 'ca': [{'00001,1': [303]}], 'right': [{'00001,1': [304]}], 'reserv': [{'00001,1': [305]}], 'main': [{'00001,1': [306]}], 'diflucan': [{'00001,1': [307]}], 'infant': [{'00001,1': [308]}], 'rss': [{'00001,1': [309]}]}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{PROCESSED_DOCS_PATH}/docID_00001_processed\", 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "    doc_num = re.findall(r\"[0-9]+\", \"docID_00001_processed\")[0]\n",
    "    all_tokens_dict = {}\n",
    "    doc_freq = 0\n",
    "\n",
    "    tokenized = word_tokenizer.tokenize(contents)\n",
    "    tokenized_and_rm_stopwords_and_stemmed = [stemmer.stem(word) for word in tokenized if word not in stopwords.words('english') and not stemmer.stem(word).isnumeric()]\n",
    "    tokens_dict = Counter(tokenized_and_rm_stopwords_and_stemmed)\n",
    "\n",
    "    distinct_tokens = tokens_dict.keys()\n",
    "\n",
    "    # output format\n",
    "    # d1 = {\n",
    "    #  'salaka': [\n",
    "    #     {'00001,2': [4, 35]}\n",
    "    #     ],\n",
    "    #  'time': [\n",
    "    #     {'00001,2': [9, 98]}\n",
    "    #     ]\n",
    "    # }\n",
    "    for term in distinct_tokens:\n",
    "        pos_list = [i for i, x in enumerate(tokenized_and_rm_stopwords_and_stemmed) if x == term]\n",
    "        term_freq = len(pos_list)\n",
    "        inner_key_format = f\"{doc_num},{term_freq}\"\n",
    "\n",
    "        if term not in all_tokens_dict:\n",
    "            all_tokens_dict[term] = []\n",
    "\n",
    "        all_tokens_dict[term].append({inner_key_format: pos_list})\n",
    "\n",
    "print(all_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21f0c61f3c72e40288537b4a5fb76c5c12af3776818c4db67cb747561ec2a074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
